{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Generative vs. Geometric: A Comparative Analysis\n",
                "\n",
                "This notebook implements and compares two information retrieval models on the **Cranfield Dataset**:\n",
                "1.  **Vector Space Model (VSM):** Using TF-IDF and Cosine Similarity.\n",
                "2.  **Language Model (LM):** Using Dirichlet Smoothing.\n",
                "\n",
                "Dataset: **Cranfield** (Aerodynamics)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import re\n",
                "import numpy as np\n",
                "from collections import defaultdict, Counter\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.metrics.pairwise import cosine_similarity"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configuration and Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Configuration ---\n",
                "DATA_DIR = os.path.join(os.getcwd(), 'cran_data')\n",
                "FILES = {\n",
                "    'docs': os.path.join(DATA_DIR, 'cran.all.1400'),\n",
                "    'queries': os.path.join(DATA_DIR, 'cran.qry'),\n",
                "    'rels': os.path.join(DATA_DIR, 'cranqrel')\n",
                "}\n",
                "RESULT_FILE = 'cran_result/cran_result.txt'\n",
                "SAMPLE_RESULT_FILE = 'cran_result/cran_sample_result.txt'"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase A: Preprocessing (The Parser)\n",
                "We handle the specific format of Cranfield files (.I, .T, .W, .A)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CranfieldParser:\n",
                "    def parse_docs(self, file_path):\n",
                "        docs = {}\n",
                "        doc_id = None\n",
                "        content = []\n",
                "        capture = False\n",
                "        \n",
                "        with open(file_path, 'r') as f:\n",
                "            lines = f.readlines()\n",
                "            \n",
                "        for line in lines:\n",
                "            line = line.strip()\n",
                "            if line.startswith('.I'):\n",
                "                if doc_id:\n",
                "                    docs[doc_id] = \" \".join(content)\n",
                "                doc_id = int(line.split()[1])\n",
                "                content = []\n",
                "                capture = False\n",
                "            elif line.startswith('.T'):\n",
                "                capture = True\n",
                "                if len(line) > 3: content.append(line[3:].strip())\n",
                "            elif line.startswith('.W'):\n",
                "                capture = True\n",
                "            elif line.startswith('.'):\n",
                "                capture = False\n",
                "            elif capture and doc_id:\n",
                "                content.append(line)\n",
                "        \n",
                "        if doc_id:\n",
                "            docs[doc_id] = \" \".join(content)\n",
                "        return docs\n",
                "\n",
                "    def parse_titles(self, file_path):\n",
                "        titles = {}\n",
                "        doc_id = None\n",
                "        title_content = []\n",
                "        capture_title = False\n",
                "        \n",
                "        with open(file_path, 'r') as f:\n",
                "            lines = f.readlines()\n",
                "            \n",
                "        for line in lines:\n",
                "            line = line.strip()\n",
                "            if line.startswith('.I'):\n",
                "                if doc_id and title_content:\n",
                "                    titles[doc_id] = \" \".join(title_content)\n",
                "                doc_id = int(line.split()[1])\n",
                "                title_content = []\n",
                "                capture_title = False\n",
                "            elif line.startswith('.T'):\n",
                "                capture_title = True\n",
                "                if len(line) > 3: title_content.append(line[3:].strip())\n",
                "            elif line.startswith('.W') or (line.startswith('.') and not line.startswith('.T')):\n",
                "                capture_title = False\n",
                "            elif capture_title and doc_id:\n",
                "                title_content.append(line)\n",
                "        \n",
                "        if doc_id and title_content:\n",
                "            titles[doc_id] = \" \".join(title_content)\n",
                "        return titles\n",
                "\n",
                "    def parse_queries(self, file_path):\n",
                "        queries = {}\n",
                "        qid = None\n",
                "        content = []\n",
                "        capture = False\n",
                "        \n",
                "        with open(file_path, 'r') as f:\n",
                "            lines = f.readlines()\n",
                "            \n",
                "        for line in lines:\n",
                "            line = line.strip()\n",
                "            if line.startswith('.I'):\n",
                "                if qid:\n",
                "                    queries[qid] = \" \".join(content)\n",
                "                qid = int(line.split()[1])\n",
                "                content = []\n",
                "                capture = False\n",
                "            elif line.startswith('.W'):\n",
                "                capture = True\n",
                "            elif line.startswith('.'):\n",
                "                capture = False\n",
                "            elif capture and qid:\n",
                "                content.append(line)\n",
                "        \n",
                "        if qid:\n",
                "             queries[qid] = \" \".join(content)\n",
                "        return queries\n",
                "\n",
                "    def parse_rels(self, file_path):\n",
                "        rels = defaultdict(set)\n",
                "        with open(file_path, 'r') as f:\n",
                "            for line in f:\n",
                "                parts = line.strip().split()\n",
                "                if not parts: continue\n",
                "                try:\n",
                "                    qid = int(parts[0])\n",
                "                    doc_id = int(parts[1])\n",
                "                    rel = int(parts[2])\n",
                "                    if rel in [1, 2, 3, 4]:\n",
                "                        rels[qid].add(doc_id)\n",
                "                except (ValueError, IndexError):\n",
                "                    continue\n",
                "        return rels"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase B: Retrieval Models - Vector Space"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "class VectorSpaceModel:\n",
                "    def __init__(self, docs):\n",
                "        self.doc_ids = list(docs.keys())\n",
                "        self.corpus = list(docs.values())\n",
                "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
                "        self.tfidf_matrix = self.vectorizer.fit_transform(self.corpus)\n",
                "\n",
                "    def retrieve(self, query):\n",
                "        query_vec = self.vectorizer.transform([query])\n",
                "        scores = cosine_similarity(query_vec, self.tfidf_matrix).flatten()\n",
                "        ranked_indices = scores.argsort()[::-1]\n",
                "        return [(self.doc_ids[i], scores[i]) for i in ranked_indices[:100]]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase B: Retrieval Models - Language Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LanguageModel:\n",
                "    def __init__(self, docs):\n",
                "        self.doc_ids = list(docs.keys())\n",
                "        self.docs = docs\n",
                "        self.vocab = set()\n",
                "        self.doc_term_counts = {}\n",
                "        self.coll_term_counts = Counter()\n",
                "        self.coll_length = 0\n",
                "        \n",
                "        print(f\"Building LM Index for {len(docs)} docs...\")\n",
                "        for doc_id, text in docs.items():\n",
                "            terms = self._tokenize(text)\n",
                "            term_counts = Counter(terms)\n",
                "            self.doc_term_counts[doc_id] = term_counts\n",
                "            self.coll_term_counts.update(term_counts)\n",
                "            self.coll_length += len(terms)\n",
                "            self.vocab.update(terms)\n",
                "\n",
                "    def _tokenize(self, text):\n",
                "        return re.findall(r'\\w+', text.lower())\n",
                "\n",
                "    def retrieve(self, query, mu=500):\n",
                "        query_terms = self._tokenize(query)\n",
                "        scores = []\n",
                "        \n",
                "        coll_probs = {}\n",
                "        for term in query_terms:\n",
                "            if term in self.vocab:\n",
                "                coll_probs[term] = self.coll_term_counts[term] / self.coll_length\n",
                "            else:\n",
                "                coll_probs[term] = 0.0\n",
                "\n",
                "        for doc_id in self.doc_ids:\n",
                "            score = 0\n",
                "            doc_len = sum(self.doc_term_counts[doc_id].values())\n",
                "            if doc_len == 0: doc_len = 0.0001 \n",
                "\n",
                "            for term in query_terms:\n",
                "                if term not in self.vocab: continue\n",
                "                \n",
                "                tf = self.doc_term_counts[doc_id][term]\n",
                "                cf_prob = coll_probs[term]\n",
                "                \n",
                "                numerator = tf + mu * cf_prob\n",
                "                denominator = doc_len + mu\n",
                "                \n",
                "                if numerator > 0:\n",
                "                    score += np.log(numerator / denominator)\n",
                "                else:\n",
                "                    score += -100\n",
                "\n",
                "            scores.append((doc_id, score))\n",
                "            \n",
                "        return sorted(scores, key=lambda x: x[1], reverse=True)[:100]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase C: Evaluation Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "def calculate_map(retrieved, relevant):\n",
                "    if not relevant: return 0.0\n",
                "    score = 0.0\n",
                "    hits = 0.0\n",
                "    for i, (doc_id, _) in enumerate(retrieved):\n",
                "        if doc_id in relevant:\n",
                "            hits += 1.0\n",
                "            score += hits / (i + 1)\n",
                "    return score / len(relevant)\n",
                "\n",
                "def calculate_p10(retrieved, relevant):\n",
                "    if not relevant: return 0.0\n",
                "    hits = sum(1 for doc_id, _ in retrieved[:10] if doc_id in relevant)\n",
                "    return hits / 10.0"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase D: Main Execution & Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Parsing Cranfield Dataset...\n",
                        "Loaded 1400 documents, 225 queries, 225 relevance sets.\n",
                        "\n",
                        "Initializing Models...\n",
                        "Building LM Index for 1400 docs...\n",
                        "\n",
                        "Running Retrieval...\n",
                        "\n",
                        "==================================================\n",
                        "       EVALUATION RESULTS        \n",
                        "==================================================\n",
                        "Model                | MAP        | P@10      \n",
                        "---------------------------------------------\n",
                        "TF-IDF (VSM)         | 0.0064     | 0.0093\n",
                        "Dirichlet LM         | 0.0049     | 0.0080\n",
                        "==================================================\n",
                        "Detailed results saved to cran_result/cran_result.txt\n",
                        "Sample detailed output saved to cran_result/cran_sample_result.txt\n"
                    ]
                }
            ],
            "source": [
                "# Execution\n",
                "if __name__ == \"__main__\":\n",
                "    parser = CranfieldParser()\n",
                "    print(\"Parsing Cranfield Dataset...\")\n",
                "    docs = parser.parse_docs(FILES['docs'])\n",
                "    titles = parser.parse_titles(FILES['docs'])\n",
                "    queries = parser.parse_queries(FILES['queries'])\n",
                "    rels = parser.parse_rels(FILES['rels'])\n",
                "    \n",
                "    print(f\"Loaded {len(docs)} documents, {len(queries)} queries, {len(rels)} relevance sets.\")\n",
                "    \n",
                "    print(\"\\nInitializing Models...\")\n",
                "    vsm = VectorSpaceModel(docs)\n",
                "    lm_model = LanguageModel(docs)\n",
                "\n",
                "    print(\"\\nRunning Retrieval...\")\n",
                "    vsm_map_scores = []\n",
                "    vsm_p10_scores = []\n",
                "    lm_map_scores = []\n",
                "    lm_p10_scores = []\n",
                "    sorted_qids = sorted(queries.keys())\n",
                "    \n",
                "    results_header = f\"{'Model':<10} | {'Query ID':<10} | {'Doc ID':<10} | {'Title':<60} | {'Query':<50}\\n\"\n",
                "    separator = \"-\" * 150 + \"\\n\"\n",
                "\n",
                "    # Open both result files\n",
                "    with open(RESULT_FILE, 'w', encoding='utf-8') as f, open(SAMPLE_RESULT_FILE, 'w', encoding='utf-8') as sample_f:\n",
                "        f.write(results_header)\n",
                "        f.write(separator)\n",
                "        \n",
                "        for i, qid in enumerate(sorted_qids):\n",
                "            query_text = queries[qid]\n",
                "            relevant = rels.get(qid, set())\n",
                "            \n",
                "            # --- VSM ---\n",
                "            results_vsm = vsm.retrieve(query_text)\n",
                "            vsm_map_scores.append(calculate_map(results_vsm, relevant))\n",
                "            vsm_p10_scores.append(calculate_p10(results_vsm, relevant))\n",
                "            \n",
                "            # Write Result Table\n",
                "            for rank, (doc_id, score) in enumerate(results_vsm[:10]):\n",
                "                title = titles.get(doc_id, \"Unknown Title\").strip().replace('\\n', ' ')\n",
                "                q_text_san = query_text.strip().replace('\\n', ' ')\n",
                "                # Truncate\n",
                "                max_len = 48\n",
                "                if len(q_text_san) > max_len:\n",
                "                    q_display = q_text_san[:max_len-3] + \"...\"\n",
                "                else:\n",
                "                    q_display = q_text_san\n",
                "                \n",
                "                f.write(f\"{'VSM':<10} | {qid:<10} | {doc_id:<10} | {title[:58]:<60} | {q_display:<50}\\n\")\n",
                "            \n",
                "            # --- LM (Mu=500) ---\n",
                "            results_lm = lm_model.retrieve(query_text, mu=500)\n",
                "            lm_map_scores.append(calculate_map(results_lm, relevant))\n",
                "            lm_p10_scores.append(calculate_p10(results_lm, relevant))\n",
                "            \n",
                "            for rank, (doc_id, score) in enumerate(results_lm[:10]):\n",
                "                title = titles.get(doc_id, \"Unknown Title\").strip().replace('\\n', ' ')\n",
                "                q_text_san = query_text.strip().replace('\\n', ' ')\n",
                "                # Truncate\n",
                "                max_len = 48\n",
                "                if len(q_text_san) > max_len:\n",
                "                    q_display = q_text_san[:max_len-3] + \"...\"\n",
                "                else:\n",
                "                    q_display = q_text_san\n",
                "                    \n",
                "                f.write(f\"{'LM':<10} | {qid:<10} | {doc_id:<10} | {title[:58]:<60} | {q_display:<50}\\n\")\n",
                "\n",
                "            # --- Sample Output (First 5 queries, Top 1 doc for each model) ---\n",
                "            if i < 5:\n",
                "                # VSM Sample\n",
                "                if results_vsm:\n",
                "                    doc_id = results_vsm[0][0]\n",
                "                    content = docs.get(doc_id, \"Content not found.\").strip()\n",
                "                    sample_f.write(f\"Model: VSM\\n\")\n",
                "                    sample_f.write(f\"query: {query_text.strip()}\\n\")\n",
                "                    sample_f.write(f\"Retrieved Document ID: {doc_id}\\n\")\n",
                "                    sample_f.write(f\"Content: {content}\\n\")\n",
                "                    sample_f.write(\"-\" * 20 + \"\\n\")\n",
                "\n",
                "                # LM Sample\n",
                "                if results_lm:\n",
                "                    doc_id = results_lm[0][0]\n",
                "                    content = docs.get(doc_id, \"Content not found.\").strip()\n",
                "                    sample_f.write(f\"Model: The Language Model\\n\")\n",
                "                    sample_f.write(f\"query: {query_text.strip()}\\n\")\n",
                "                    sample_f.write(f\"Retrieved Document ID: {doc_id}\\n\")\n",
                "                    sample_f.write(f\"Content: {content}\\n\")\n",
                "                    sample_f.write(\"=\" * 50 + \"\\n\\n\")\n",
                "    \n",
                "    print(f\"\\n\" + \"=\"*50)\n",
                "    print(f\"       EVALUATION RESULTS        \")\n",
                "    print(f\"=\"*50)\n",
                "    print(f\"{'Model':<20} | {'MAP':<10} | {'P@10':<10}\")\n",
                "    print(\"-\" * 45)\n",
                "    print(f\"{'TF-IDF (VSM)':<20} | {np.mean(vsm_map_scores):.4f}     | {np.mean(vsm_p10_scores):.4f}\")\n",
                "    print(f\"{'Dirichlet LM':<20} | {np.mean(lm_map_scores):.4f}     | {np.mean(lm_p10_scores):.4f}\")\n",
                "    print(\"=\"*50)\n",
                "    print(f\"Detailed results saved to {RESULT_FILE}\")\n",
                "    print(f\"Sample detailed output saved to {SAMPLE_RESULT_FILE}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
