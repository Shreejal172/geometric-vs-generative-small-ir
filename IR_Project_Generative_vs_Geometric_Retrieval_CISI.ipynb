{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Generative vs. Geometric: A Comparative Analysis (Backup/CISI)\n",
                "\n",
                "This notebook implements and compares two information retrieval models on the **CISI Dataset**:\n",
                "1.  **Vector Space Model (Week 3):** Using TF-IDF and Cosine Similarity.\n",
                "2.  **Query Likelihood Model (Week 5):** Using Dirichlet Smoothing.\n",
                "\n",
                "Dataset: **CISI** (Computer and Information Science)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# importing standard stuff for file handling\n",
                "import os\n",
                "import re\n",
                "# numpy is for the math heavy lifting\n",
                "import numpy as np\n",
                "from collections import defaultdict, Counter\n",
                "# using sklearn for the vector magic and similarity checks\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.metrics.pairwise import cosine_similarity"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configuration and Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confguring\n",
                "DATA_DIR = os.path.join(os.getcwd(), 'cisi_data')\n",
                "# setting up where our data lives\n",
                "FILES = {\n",
                "    'docs': os.path.join(DATA_DIR, 'CISI.ALL'),\n",
                "    'queries': os.path.join(DATA_DIR, 'CISI.QRY'),\n",
                "    'rels': os.path.join(DATA_DIR, 'CISI.REL')\n",
                "}\n",
                "RESULT_FILE = 'cisi_result/cisi_result.txt'\n",
                "SAMPLE_RESULT_FILE = 'csi_result/cisi_sample_result.txt'\n",
                "\n",
                "# list of boring words we dont care about\n",
                "STOPWORDS = set([\n",
                "    \"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"but\", \"by\", \"for\", \"if\", \"in\", \n",
                "    \"into\", \"is\", \"it\", \"no\", \"not\", \"of\", \"on\", \"or\", \"such\", \"that\", \"the\", \n",
                "    \"their\", \"then\", \"there\", \"these\", \"they\", \"this\", \"to\", \"was\", \"will\", \"with\"\n",
                "])\n",
                "\n",
                "# basic tokenizer to clean up the text junk\n",
                "def tokenize(text):\n",
                "    \"\"\"Simple tokenizer that removes punctuation and stopwords.\"\"\"\n",
                "    text = text.lower()\n",
                "    text = re.sub(r'[^a-z0-9\\s]', ' ', text) # Keep only alphanumeric and space\n",
                "    tokens = text.split()\n",
                "    return [t for t in tokens if t not in STOPWORDS]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase A: Preprocessing (The Parser)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CISIParser:\n",
                "    # helper to read the weird cisi format\n",
                "    @staticmethod\n",
                "    def _parse_cisi_content(file_path, target_tags):\n",
                "        with open(file_path, 'r') as f:\n",
                "            content = f.read()\n",
                "        \n",
                "        parsed_data = {}\n",
                "        items = content.split('.I ')\n",
                "        \n",
                "        for item in items[1:]:  # Skip empty preamble\n",
                "            lines = item.split('\\n')\n",
                "            try:\n",
                "                obj_id = int(lines[0].strip())\n",
                "            except ValueError:\n",
                "                continue # Skip malformed IDs\n",
                "            \n",
                "            collected_text = []\n",
                "            current_tag = None\n",
                "            \n",
                "            for line in lines:\n",
                "                if line.startswith('.'):\n",
                "                    # Update current state (e.g., .T, .W, .A)\n",
                "                    current_tag = line[:2]\n",
                "                    continue\n",
                "                \n",
                "                # If we are currently inside one of the tags we want, keep the line\n",
                "                if current_tag in target_tags:\n",
                "                    collected_text.append(line)\n",
                "            \n",
                "            parsed_data[obj_id] = \" \".join(collected_text).strip()\n",
                "            \n",
                "        return parsed_data\n",
                "\n",
                "    # parsing the docs specifically looking for titles and abstracts\n",
                "    @staticmethod\n",
                "    def parse_docs(file_path):\n",
                "        # Documents need Title (.T) and Abstract (.W)\n",
                "        return CISIParser._parse_cisi_content(file_path, target_tags=['.T', '.W'])\n",
                "\n",
                "    @staticmethod\n",
                "    def parse_titles(file_path):\n",
                "        # Reporting only needs Title (.T)\n",
                "        return CISIParser._parse_cisi_content(file_path, target_tags=['.T'])\n",
                "\n",
                "    # getting the queries\n",
                "    @staticmethod\n",
                "    def parse_queries(file_path):\n",
                "        # Queries usually only have body text (.W)\n",
                "        # Note: Some CISI queries have .T too, but usually .W is the core question\n",
                "        return CISIParser._parse_cisi_content(file_path, target_tags=['.W'])\n",
                "\n",
                "    # parsing the relevance judgments which is a bit different structure\n",
                "    @staticmethod\n",
                "    def parse_rels(file_path):\n",
                "        # This format is completely different (Structure: QID DOCID ...), \n",
                "        # so it stays as its own distinct logic.\n",
                "        rels = defaultdict(set)\n",
                "        with open(file_path, 'r') as f:\n",
                "            for line in f:\n",
                "                parts = line.strip().split()\n",
                "                if len(parts) >= 2:\n",
                "                    try:\n",
                "                        qid = int(parts[0])\n",
                "                        doc_id = int(parts[1])\n",
                "                        rels[qid].add(doc_id)\n",
                "                    except ValueError:\n",
                "                        continue\n",
                "        return rels\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase B: Retrieval Models - Vector Space"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "class VectorSpaceModel:\n",
                "    # class for the classic vector model\n",
                "    def __init__(self, docs):\n",
                "        self.doc_ids = list(docs.keys())\n",
                "        self.corpus = [docs[did] for did in self.doc_ids]\n",
                "        # creating the tfidf matrix here\n",
                "        self.vectorizer = TfidfVectorizer(\n",
                "            tokenizer=tokenize, \n",
                "            stop_words=None,\n",
                "            token_pattern=None\n",
                "        )\n",
                "        self.doc_vectors = self.vectorizer.fit_transform(self.corpus)\n",
                "\n",
                "    def retrieve(self, query_text):\n",
                "        q_vec = self.vectorizer.transform([query_text])\n",
                "        # comparing the query vector with all doc vectors using cosine\n",
                "        scores = cosine_similarity(q_vec, self.doc_vectors).flatten()\n",
                "        ranked_indices = scores.argsort()[::-1]\n",
                "        results = []\n",
                "        for idx in ranked_indices:\n",
                "            if scores[idx] > 0:\n",
                "                results.append((self.doc_ids[idx], scores[idx]))\n",
                "        return results"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase B: Retrieval Models - Language Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DirichletLM:\n",
                "    # language model with dirichlet smoothing\n",
                "    def __init__(self, docs, mu=100):\n",
                "        self.mu = mu\n",
                "        self.index = defaultdict(Counter)\n",
                "        self.doc_lengths = {}\n",
                "        self.total_corpus_terms = 0\n",
                "        self.doc_ids = list(docs.keys())\n",
                "        \n",
                "        print(f\"Building LM Index for {len(docs)} docs...\")\n",
                "        # building our own manual index to count stuff\n",
                "        for doc_id, text in docs.items():\n",
                "            tokens = tokenize(text)\n",
                "            length = len(tokens)\n",
                "            self.doc_lengths[doc_id] = length\n",
                "            self.total_corpus_terms += length\n",
                "            for t in tokens:\n",
                "                self.index[t][doc_id] += 1\n",
                "\n",
                "    def retrieve(self, query_text):\n",
                "        query_tokens = tokenize(query_text)\n",
                "        scores = []\n",
                "        \n",
                "        query_stats = {}\n",
                "        for token in query_tokens:\n",
                "             if token in self.index:\n",
                "                 query_stats[token] = sum(self.index[token].values())\n",
                "\n",
                "        for doc_id in self.doc_ids:\n",
                "            doc_len = self.doc_lengths.get(doc_id, 0)\n",
                "            if doc_len == 0:\n",
                "                scores.append((doc_id, -float('inf')))\n",
                "                continue\n",
                "            \n",
                "            score = 0.0\n",
                "            for token in query_tokens:\n",
                "                # calculating the score based on probability\n",
                "                if token not in query_stats: continue \n",
                "                tf = self.index[token].get(doc_id, 0)\n",
                "                # P(w|C)\n",
                "                corpus_tf = query_stats[token]\n",
                "                p_C = corpus_tf / self.total_corpus_terms\n",
                "                \n",
                "                # smoothing bit to handle unseen words\n",
                "                numerator = tf + (self.mu * p_C)\n",
                "                denominator = doc_len + self.mu\n",
                "                score += np.log(numerator / denominator)\n",
                "            scores.append((doc_id, score))\n",
                "            \n",
                "        return sorted(scores, key=lambda x: x[1], reverse=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase C: Evaluation Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# calculating mean average precision because it is standard\n",
                "def calculate_map(retrieved, relevant):\n",
                "    if not relevant: return 0.0\n",
                "    score = 0.0\n",
                "    hits = 0.0\n",
                "    for i, (doc_id, _) in enumerate(retrieved):\n",
                "        if doc_id in relevant:\n",
                "            hits += 1.0\n",
                "            score += hits / (i + 1)\n",
                "    return score / len(relevant)\n",
                "\n",
                "# checking how many good ones are in the top 10\n",
                "def calculate_p10(retrieved, relevant):\n",
                "    if not relevant: return 0.0\n",
                "    hits = sum(1 for doc_id, _ in retrieved[:10] if doc_id in relevant)\n",
                "    return hits / 10.0"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase D: Main Execution & Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Parsing CISI Dataset...\n",
                        "Loaded 1460 documents, 112 queries, 76 relevance sets.\n",
                        "\n",
                        "Initializing Models...\n",
                        "Building LM Index for 1460 docs...\n",
                        "\n",
                        "Running Retrieval...\n",
                        "\n",
                        "========================================\n",
                        "       EVALUATION RESULTS        \n",
                        "========================================\n",
                        "Model                | MAP        | P@10      \n",
                        "----------------------------------------------\n",
                        "TF-IDF               | 0.1908     | 0.3013\n",
                        "Dirichlet LM         | 0.1900     | 0.2711\n",
                        "========================================\n",
                        "Detailed results saved to csi_result/cisi_result.txt\n",
                        "Sample detailed output saved to csi_result/cisi_sample_result.txt\n"
                    ]
                }
            ],
            "source": [
                "# Execution\n",
                "if __name__ == \"__main__\":\n",
                "    print(\"Parsing CISI Dataset...\")\n",
                "    # loading everything up\n",
                "    docs = CISIParser.parse_docs(FILES['docs'])\n",
                "\n",
                "    titles = CISIParser.parse_titles(FILES['docs'])\n",
                "    queries = CISIParser.parse_queries(FILES['queries'])\n",
                "    rels = CISIParser.parse_rels(FILES['rels'])\n",
                "    \n",
                "    print(f\"Loaded {len(docs)} documents, {len(queries)} queries, {len(rels)} relevance sets.\")\n",
                "    \n",
                "    print(\"\\nInitializing Models...\")\n",
                "    # initializing our two contenders\n",
                "    vsm = VectorSpaceModel(docs)\n",
                "    lm = DirichletLM(docs, mu=4000)\n",
                "\n",
                "    print(\"\\nRunning Retrieval...\")\n",
                "    # running the retrieval loop\n",
                "    vsm_map_scores, vsm_p10_scores = [], []\n",
                "    lm_map_scores, lm_p10_scores = [], []\n",
                "    \n",
                "    active_queries = sorted([qid for qid in queries if qid in rels])\n",
                "    \n",
                "    results_header = f\"{'Model':<10} | {'Query ID':<10} | {'Doc ID':<10} | {'Title':<60} | {'Query':<50}\\n\"\n",
                "    separator = \"-\" * 150 + \"\\n\"\n",
                "\n",
                "    with open(RESULT_FILE, 'w', encoding='utf-8') as f, open(SAMPLE_RESULT_FILE, 'w', encoding='utf-8') as sample_f:\n",
                "        f.write(results_header)\n",
                "        f.write(separator)\n",
                "\n",
                "        for i, qid in enumerate(active_queries):\n",
                "            query_text = queries[qid]\n",
                "            relevant = rels[qid]\n",
                "            \n",
                "            # --- VSM ---\n",
                "            results_vsm = vsm.retrieve(query_text)\n",
                "            vsm_map_scores.append(calculate_map(results_vsm, relevant))\n",
                "            vsm_p10_scores.append(calculate_p10(results_vsm, relevant))\n",
                "            \n",
                "            for doc_id, score in results_vsm[:10]:\n",
                "                title = titles.get(doc_id, \"Unknown Title\").strip().replace('\\n', ' ')\n",
                "                q_text_san = query_text.strip().replace('\\n', ' ')\n",
                "                # Truncate\n",
                "                max_len = 48\n",
                "                if len(q_text_san) > max_len:\n",
                "                    q_display = q_text_san[:max_len-3] + \"...\"\n",
                "                else:\n",
                "                    q_display = q_text_san\n",
                "                f.write(f\"{'VSM':<10} | {qid:<10} | {doc_id:<10} | {title[:58]:<60} | {q_display:<50}\\n\")\n",
                "\n",
                "            # --- LM ---\n",
                "            results_lm = lm.retrieve(query_text)\n",
                "            lm_map_scores.append(calculate_map(results_lm, relevant))\n",
                "            lm_p10_scores.append(calculate_p10(results_lm, relevant))\n",
                "            \n",
                "            for doc_id, score in results_lm[:10]:\n",
                "                title = titles.get(doc_id, \"Unknown Title\").strip().replace('\\n', ' ')\n",
                "                q_text_san = query_text.strip().replace('\\n', ' ')\n",
                "                # Truncate\n",
                "                max_len = 48\n",
                "                if len(q_text_san) > max_len:\n",
                "                    q_display = q_text_san[:max_len-3] + \"...\"\n",
                "                else:\n",
                "                    q_display = q_text_san\n",
                "                f.write(f\"{'LM':<10} | {qid:<10} | {doc_id:<10} | {title[:58]:<60} | {q_display:<50}\\n\")\n",
                "\n",
                "            # Samples (First 5 queries) \n",
                "            # saving specific examples to check later\n",
                "            if i < 5:\n",
                "                # VSM Sample\n",
                "                if results_vsm:\n",
                "                    doc_id = results_vsm[0][0]\n",
                "                    content = docs.get(doc_id, \"Content not found.\").strip()\n",
                "                    sample_f.write(f\"Model: VSM\\n\")\n",
                "                    sample_f.write(f\"query: {query_text.strip()}\\n\")\n",
                "                    sample_f.write(f\"Retrieved Document ID: {doc_id}\\n\")\n",
                "                    sample_f.write(f\"Content: {content}\\n\")\n",
                "                    sample_f.write(\"-\" * 20 + \"\\n\")\n",
                "                # LM Sample\n",
                "                if results_lm:\n",
                "                    doc_id = results_lm[0][0]\n",
                "                    content = docs.get(doc_id, \"Content not found.\").strip()\n",
                "                    sample_f.write(f\"Model: The Language Model\\n\")\n",
                "                    sample_f.write(f\"query: {query_text.strip()}\\n\")\n",
                "                    sample_f.write(f\"Retrieved Document ID: {doc_id}\\n\")\n",
                "                    sample_f.write(f\"Content: {content}\\n\")\n",
                "                    sample_f.write(\"=\" * 50 + \"\\n\\n\")\n",
                "    \n",
                "    print(f\"\\n\" + \"=\"*40)\n",
                "    print(f\"       EVALUATION RESULTS        \")\n",
                "    print(f\"=\"*40)\n",
                "    print(f\"{'Model':<20} | {'MAP':<10} | {'P@10':<10}\")\n",
                "    print(\"-\" * 46)\n",
                "    print(f\"{'TF-IDF ':<20} | {np.mean(vsm_map_scores):.4f}     | {np.mean(vsm_p10_scores):.4f}\")\n",
                "    print(f\"{'Dirichlet LM ':<20} | {np.mean(lm_map_scores):.4f}     | {np.mean(lm_p10_scores):.4f}\")\n",
                "    print(\"=\"*40)\n",
                "    print(f\"Detailed results saved to {RESULT_FILE}\")\n",
                "    print(f\"Sample detailed output saved to {SAMPLE_RESULT_FILE}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
